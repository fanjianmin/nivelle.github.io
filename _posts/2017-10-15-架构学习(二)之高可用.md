---
layout: post
title:  "架构学习(二)之高可用"
date:   2017-10-14 00:06:05
categories: 架构
tags: 架构
excerpt: 架构
author: nivelle
---

* content
{:toc}


---
layout: post
title:  "高可用"
date:   2017-10-14 00:06:05
categories: 架构
tags: 架构
excerpt: 架构
author: nivelle
---

* content
{:toc}


### 高可用

#### 负载均衡与反向代理

对于一般应用,有Nginx一般用于七层负载均衡,其吞吐量是有一定限制的.为了提升整体吞吐量,会在DNS和Nginx之间引入接入层,如果使用LVS(软件负载均衡器),F5(硬负载均衡器)可以做四层负载均衡,即首先DNS解析到LVS/F5,然后LVS/F5转发给Nginx,再由Nginx转发给后端Real Server.

![image](http://7xpuj1.com1.z0.glb.clouddn.com/%E5%9B%9B%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1.png)


LVS DR的工工作模式:工作在数据链路层,LVS和上游服务器共享同一个VIP,通过改写报文的目标MAC地址为上游服务器MAC地址实现负载均衡,上游服务器直接响应报文到客户端,不经过LVS,从而提升性能.但因为LVS和上游服务器必须在同一个子网,为了解决子网问题而不影响负载性能,可以在在LVS后边挂HaProxy,通过四到七层负载均衡器HaProxy集群来解决跨网和性能问题.

二层负载是通过改写报文的目标MAC地址为上游服务器MAC地址,源IP地址和目标IP地址是没有改变的,负载均衡服务器和真实服务器共享同一个VIP,如LVS DR工作模式.


**术语介绍:**

1. 上游服务器配置:使用upstream server 配置上游服务器,指Nginx负载均衡到处理业务的服务器,也可以称为real server,即真实处理业务的服务器
2. 负载均很算法:配置多个上游服务器时的负载均衡机制
3. 失败重试机制:配置当超时或上游服务器不存活时,是否需要重试其他上游服务器.
4. 服务器心跳检测:上游服务器的健康检查/心跳检查

- upstream 配置

我们需要给Nginx配置上游服务器,即负载均衡到的真实处理业务的服务器,通过在http指令下配置upstram即可.

```
 upstream backend{
     server 192.168.61.1:9080 weight=1;
     server 192.168.61.1:9090 weight=2;
 }
```
upstram server的主要配置如下:

1. ip地址和端口:配置上游服务器的IP地址和端口
2. 权重:weight用来配置权重,默认都是1,权重越高分配给这台服务器的请求就越多.(如上配置为每三次请求转发给9080,其余两个转发给9090),需要根据服务器的实际处理能力设置权重.

然后,我们可以配置如下proxy_pass来处理用户请求.

```
location /{
    proxy_pass http"//backend;
}

```
当访问Nginx时,会将请求反向代理到backend配置的upstream server.

- 负载均衡算法:负载均衡用来解决用户请求到来时如何选择upstream server进行处理,默认采用的是round-robin(轮询),同时他支持几种算法:
  
  1. round0robin:轮询,默认负载均和算法,即以轮询的方式将请求转发到上游服务器,通过配合weight配置可以实现基于权重的轮询
  2. ip_hash:根据客户端IP进行负载均衡,即相同的IP将负载均衡到同一个upstream server.
  
  3. hash key[consistent]:对某一个key进行或者使用一致性哈希算法进行负载均衡.使用Hash算法存在的问题是,当添加或者删除一台服务器时,将导致很多key被重新负载到不同的服务器;因此,建议考虑一致性哈希算法,这样当添加/删除一台服务器时,只有少数key将被重新负载均衡到不同的服务器.
  4. 哈希算法:此初是根据请求uri进行负载均衡,可以使用Nginx变量,因此,可以实现复杂的算法.
  5. 一致性哈希算法:consistent_key动态指定.
  6. least_conn:将请求负载均衡到最少活跃连接的上游服务器.如果配置的服务器较少,则将转而使用基于权重的轮询算法.


- 失败重试:主要有两部分配置:upstream server 和 proxy_pass

```
upstream backend{
    server 192.168.61.1:9080 max_fails =2 fail_timeout =10s weight=1;
    server 192.168.61.1:9090 max_fails =2 fail_timeout=10s weight=1;
}

```

通过配置上游服务器的 max_fails 和 fail_timeout ,来指定每个上游服务器,当fail_timeout 时间内失败了max_fails 次请求,则认为不可用.然后将摘掉该上游服务器,fail_timeout时间后会再次将该服务器加入到存活上游服务器列表进行重试.

- 健康检查:Nginx对上游服务器的健康检查默认采用的是惰性策略,Nginx商业版提供了health_check进行主动健康检查.当然也可以集成nginx_upsream_check_module模块来进行主动健康检查,它支持TCP心跳和Http心跳来实现健康检查.
  
  - TCP心跳检查:
  ```
  upsream backend{
      server 192.168.61.1:9080 weight =1;
      server 192.168.61.1:9090 weight=2;
      check interval =3000 rise=1 fall=3 timeout=2000 type = tcp
  }
  
  ```
  1. interval:检测时间间隔,此处配置了每隔3s检测一次
  2. fall:检测失败多少次后,上游服务器被标识为不存活
  3. rise:检测成功多少次后,上游服务器被标识为存活,并可以处理请求
  4. timeout:检测请求超时时间配置

  - Http心跳检查
  ```
   upsream backend{
      server 192.168.61.1:9080 weight =1;
      server 192.168.61.1:9090 weight=2;
      check interval =3000 rise=1 fall=3 timeout=2000 type = http;
      check_http_send "HEAD/status HTTP/1.0\r\n\r\n";
      check_http_expect_alive http_2xx http_3xx;
  }
  
  ```
  1. check_http_send:即检查时发的HTTP请求内容
  2. check_http_expect_alive:当上游服务器返回匹配的响应状态码时,则认为上游服务器存活


